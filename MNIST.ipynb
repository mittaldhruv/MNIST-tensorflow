{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Say we have an image of 28x28 i.e. 784 features, then if we add a hidden layer with even 100 units, we will have 78400 weights and updating so many weights would be hectic and would result in overfitting. If this image is much bigger then it would be worse. So we have to avoid this, we will use a method of convolution layer\n",
    "    \n",
    "    Concepts Used:\n",
    "        Padding\n",
    "        Stride (how much gap)\n",
    "        Forming Channels\n",
    "        Pooling (max pooling works much better)\n",
    "        Stretching (i.e. these pixels after pooling will be considered as new features and will pass through hidden layer and further)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 784\n",
    "n_classes = 10\n",
    "learning_rate = 0.01\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    weights of c1 has 4 values: rows, columns, depth, n of units in that layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'c1' : tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    'c2' : tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'h1' : tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out' : tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'c1' : tf.Variable(tf.random_normal([32])),\n",
    "    'c2' : tf.Variable(tf.random_normal([64])),\n",
    "    'h1' : tf.Variable(tf.random_normal([1024])),\n",
    "    'out' : tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(x, weights, biases, strides = 1):\n",
    "    x = tf.nn.conv2d(x, weights, padding = 'SAME', strides = [1, strides, strides, 1], use_cudnn_on_gpu = True)    # this is inbuilt func, but it doesn't include biases, strides has some values, whose first value is 1 and last value(depth) will be 1\n",
    "    x = tf.nn.bias_add(x, biases)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpooling(conv, k):\n",
    "    return tf.nn.max_pool(conv, padding = 'SAME', ksize = [1, k, k, 1], strides = [1, k, k, 1])\n",
    "    \n",
    "def cnn_result(x, weights, biases, keep_prob):\n",
    "    x = tf.reshape(x, shape = [-1, 28, 28, 1])\n",
    "    \n",
    "    conv1 = conv(x, weights['c1'], biases['c1'])\n",
    "    conv1 = maxpooling(conv1, k = 2)    # here k = pool size\n",
    "    \n",
    "    conv2 = conv(conv1, weights['c2'], biases['c2'])\n",
    "    conv2 = maxpooling(conv2, k = 2)\n",
    "    \n",
    "    hidden_input = tf.reshape(conv2, shape = [-1, 7 * 7 * 64])\n",
    "    hidden_output_before_relu = tf.add(tf.matmul(hidden_input, weights['h1']), biases['h1'])\n",
    "    hidden_output = tf.nn.relu(hidden_output_before_relu)\n",
    "    \n",
    "        # dropout those outputs, \n",
    "    hidden_output = tf.nn.dropout(hidden_output, keep_prob)\n",
    "    \n",
    "    out_layer = tf.matmul(hidden_output, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = cnn_result(x, weights, biases, 0.75)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "933798.720995\n",
      "39221.2301272\n",
      "22627.1915514\n",
      "15456.2256817\n",
      "11103.4317173\n",
      "10113.4105084\n",
      "7423.04772268\n",
      "6493.56750507\n",
      "5275.32351841\n",
      "6065.99272602\n",
      "5444.98623747\n",
      "3687.26005494\n",
      "3681.52995338\n",
      "3585.88074394\n",
      "3145.10566601\n",
      "3025.78951702\n",
      "2433.82805677\n",
      "2327.4125915\n",
      "2595.47355897\n",
      "2580.50582671\n",
      "1776.57954914\n",
      "2175.72579796\n",
      "2055.79514906\n",
      "2109.00953255\n",
      "1327.20202589\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "batch_size = 100\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(25):\n",
    "    num_batches = int(mnist.train.num_examples/batch_size)\n",
    "    total_cost = 0\n",
    "    for j in range(num_batches):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        _,c = sess.run([optimizer, cost], feed_dict = {x:batch_x, y:batch_y})\n",
    "        total_cost += c\n",
    "    print (total_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ...,  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "predictions = sess.run(correct_prediction, feed_dict = {x:mnist.test.images, y:mnist.test.labels})\n",
    "predictions.sort()\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9967636363636364\n"
     ]
    }
   ],
   "source": [
    "print ('accuracy = ', (55000-178)/55000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-gpu]",
   "language": "python",
   "name": "conda-env-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
